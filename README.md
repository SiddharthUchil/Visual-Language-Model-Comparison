# Visual Language Model Comparison: Pushing the Boundaries of Multimodal AI

https://github.com/SiddharthUchil/Visual-Language-Model-Comparison/assets/36127139/5d1cdbf0-0158-432d-94a7-e8b93e6685b4

This GitHub project aims to provide a comprehensive analysis and evaluation of the most advanced visual language models (VLMs) currently available. VLMs represent a groundbreaking development in artificial intelligence, enabling seamless integration of text and image understanding, unlocking a myriad of exciting applications that were previously unattainable.

## Project Overview

The primary objective of this project is to conduct a rigorous comparison of the leading VLMs, including GPT-4 Vision, Gemini Pro Vision, Claude 3 Opus, and Claude Haiku. By leveraging the powerful RAGS (Robust AI Grounding System) evaluation framework, we meticulously assess the performance and capabilities of these cutting-edge models across various tasks and scenarios.

## Evaluation Methodology

To ensure a fair and consistent evaluation, a carefully curated set of images and corresponding questions were presented to each VLM. The images covered a diverse range of topics, including financial projections, nutrition labels, actor networks, celebrity identification, concept diagrams, product listings, license plate recognition, and text extraction from billboards. The images and questions were structured as follows:

| Images                | Questions                                                                                     |
|-----------------------|-----------------------------------------------------------------------------------------------|
| financial_projections | What is our projected revenue for 2018, and are we profitable?                                |
| soda_nutrition        | Give me the complete nutritional facts, grams, and % daily value                               |
| actors_graph          | Which two actors had the most movies in common?                                               |
| lex_yann image        | Who are the two famous people in this image?                                                   |
| lda_model             | What concept is explained in this diagram?                                                     |
| amazon_items          | What is the cheapest item, and the most expensive item in this picture?                         |
| license_plate         | What's the license of the main car in the image?                                                |
| street_night          | What does the text say in the longest billboard in the image?                                    |
| contrastive_learning  | What concept is explained in this diagram?                                                      |

By presenting the same set of images and questions to each VLM, the project aims to provide a comprehensive and unbiased evaluation of their respective capabilities, enabling a direct comparison of their performance across various multimodal tasks.

## Key Features

- **Multimodal Reasoning:** Explore the ability of VLMs to comprehend and reason over both textual and visual data, enabling sophisticated multimodal understanding and decision-making.
- **Comprehensive Evaluation:** Utilize RAGS, a state-of-the-art evaluation framework, to rigorously assess the performance of VLMs across a diverse range of tasks, including image captioning, visual question answering, and multimodal reasoning.
- **Streamlit Dashboard:** Leverage the power of Streamlit to create an interactive and user-friendly dashboard, allowing for seamless visualization and exploration of evaluation results, facilitating in-depth analysis and comparison of the VLMs' capabilities.
- **Reproducibility:** Ensure transparency and reproducibility by providing detailed documentation, code, and data, enabling researchers and practitioners to replicate and build upon the findings of this project.
- **Cutting-Edge Research:** Contribute to the rapidly evolving field of multimodal AI by exploring the latest advancements in VLMs and their potential applications in various domains, such as healthcare, education, and creative industries.

## Technical Aspects

- This project leverages state-of-the-art deep learning techniques, including transformer architectures, attention mechanisms, and multimodal fusion strategies, to enable VLMs to effectively process and integrate textual and visual information.
- The evaluation process involves carefully curated datasets and benchmarks, designed to challenge the VLMs' abilities in tasks such as image captioning, visual question answering, and multimodal reasoning. Advanced metrics, including BLEU, METEOR, and CIDEr, are employed to quantify the models' performance accurately.
- The Streamlit dashboard incorporates interactive visualizations, such as confusion matrices, precision-recall curves, and attention heatmaps, to provide intuitive insights into the models' strengths, weaknesses, and decision-making processes.


